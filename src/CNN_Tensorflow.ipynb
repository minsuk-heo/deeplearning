{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (Convolutional Neural Network)\n",
    "We will implement very basic CNN with MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why CNN for image recognition?\n",
    "Think about you use MLP (multi layer perceptron) for image recognition.  \n",
    "You can recall how MLP works from below picture.  \n",
    "MLP changes image shape from 2d(matrix) to 1d(array) and fully connect all nodes to get prediction.  \n",
    "Take a moment to see difference between two different inputs in which both target numbers are equally '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mlp_overview.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mlp_overview.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mlp_overview2.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mlp_overview2.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though they have same target number('2'), as you can see the **input arrays are very different.**  \n",
    "Since MLP only knows input array, it is hard for MLP to learn these two numbers have same target.  \n",
    "As a solution for this, you can introduce more nodes in the MLP, but this solution easily results in high computation, longer train.\n",
    "\n",
    "MLP's issue:  \n",
    "1) when number is written in different pixels, input for MLP is totally different.    \n",
    "2) having many nodes results in high computation and longer train time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you recognize a thing?\n",
    "In order to resolve MLP's issue, let's think how we actually recognize a number.  \n",
    "If you look at number '2', how do you know it is 2?  \n",
    "Your brain consciously or inconsciously recognize head and tail of number 2 and diagonal connector between head and tail, just like below pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/cnn_filter.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/cnn_filter.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN is recognizing a thing just like you!\n",
    "CNN recognize an object just like you. CNN is trained to capture features of object (in this case, head, tail, connector, edge) and recognize the number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How CNN identify features?\n",
    "In contrast to MLP, CNN uses 2 dimension information in order to capture local connectivity such as head, tail, connector.  \n",
    "\n",
    "here is how CNN finds features of object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/stride.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/stride.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stride, filter, receptive field\n",
    "As you can see from above picture, CNN slides filter from the top left to the bottom right.  \n",
    "Sliding filter is called **stride**.    \n",
    "**Filter** is feature identifier, sometimes called **kernel**,  \n",
    "and we call the area where filter stays a **receptive field**.  \n",
    "the filter from above picture is to identify diagonal connected pixel.  \n",
    "\n",
    "From below picture, you can see the filter detects two diagonal connected pixels from left input (digit 2),  \n",
    "while there is no diagonal connected pixels frin right input (digit 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/filter_diff.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/filter_diff.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know CNN is just mathmetical model, we haven't talk math behind intuitive example.  \n",
    "Suppose our image is gray scale image, the pixel will be represented number between 0 to 255.  \n",
    "0 means white color and 255 means black color just like below picture.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/elem_mul.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/elem_mul.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above pictures, the filter detects feature by element-wise multiplication of filter and receptive field numbers.  \n",
    "Larger number means the area has more chance to have the feature,  \n",
    "Less number means the area has less chance to have the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect your target, for example, here to detect number between 0 to 9,  \n",
    "you will need more filters such as straight line, vertical line, curve line.  \n",
    "your **first convolutional layer** will detect these essential features and  \n",
    "the **next convolutional layers** can detect more high level such as circle, triangle, rectangle based on the previous detected features.  \n",
    "After detecting all features, these features will be input to MLP (full connected layers) and end up with softmax to classify input to target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is one of famous CNN architecture from Stanford CNN lecture slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/cnn_architecture.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/cnn_architecture.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "citation:\n",
    "source from http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONV stands for convolutioal layer which we have learned so far.  \n",
    "Also you can find FC which is fully connected layer for classifying input to given target.\n",
    "In this picture, you can see RELU as activation function of convolutional layer. we will also use RELU for our tensorflow practice in this jupyter notebook.  \n",
    "\n",
    "The only one term we haven't talk is POOL which is pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pulling layer\n",
    "The main purpose of having pulling layer is to reduce parameters and computtations of CNN model.  \n",
    "Therefore, controlling overfitting.\n",
    "\n",
    "From our stride example of digit 2 (when stride is 1),  \n",
    "the output of stride of the diagonal filter (we call it **feature map**) is below,  \n",
    "FYI, Relu(feature map) is called a **actvation map**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/stride_result.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/stride_result.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you apply pooling, you can use either **max pooling** or **average pooling**.  \n",
    "Here is an example of max pooling (2*2 filter and stride 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/max_pool.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/max_pool.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output of convolutional layer decreased from (4 x 4) to (2 x 2).  \n",
    "Decreasing feature map results in reducing the number of parameters and computation time.  \n",
    "Also by reducing the number of parameters, it gives control of **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero padding\n",
    "Lastly, I should give you insights of zero padding while it is not shown at Stanford CNN architecture diagram.  \n",
    "Zero padding is mostly applied on recent CNN with mainly two reasons below,  \n",
    "1) reduce information loss from convolutional layer.   \n",
    "2) let the CNN knows where is the boundary of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit convolutional layer. As you can see from below image,  \n",
    "The output dimension (4 x 4) is less than input dimension (5 x 5). That said, we lose some info at each convonlutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/zeropadding1.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/zeropadding1.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look below picture. Zero Padding gives more space to stride.  \n",
    "If your filter size is (3 X 3) and stride 1 pixel at a time, the output dimension will be (5 x 5)  \n",
    "which is exactly same with original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/zeropadding.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/zeropadding.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what if image is color image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, I use MNIST which is gray scale image, however there are a lot more chance you will handle color image which is composed with RGB color. This is very easy problem if you think color image is just overlapped three layers.  \n",
    "if you have 10 gray scale image which is 28 * 28 pixel, your input tensor is (10,28,28,1). because you only have one color.  \n",
    "if you have 10 RGB color image which is 28 * 28 pixel, your input tensor is (10,28,28,3).  because you have three colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/rgb.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/rgb.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above picture, you can double check the color image is just overlapped three color layers.  \n",
    "We call the overlapped layers a **depth**.\n",
    "let's take a look how one filter work on three color layers from below image.  \n",
    "You can find one filter has three sub filters. sub filters will stride and each stide will be summed up (plus bias) to one pixel in a feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/rgb1.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/rgb1.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/rgb2.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/rgb2.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above picture, each filter output is one feature map.  \n",
    "Therefore, if you have 10 filters at first convolutional layer, the next layer's input will have **depth** 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Let's resume entire CNN architecture to talk about Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/cnn_train.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/cnn_train.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above picture, if input is color image, the depth is 3, that is why you can see three layer at the input.  \n",
    "First Convolution layer has four filters that is why you have four layers at Conv1.  \n",
    "Second Convolution layer has three filters that is why you have three layers at Conv2.\n",
    "Pooling layer has stride size 2, that is why you have (2 x 2) feature maps.  \n",
    "Flatten will have 2 x 2 x 3 = 12 values in an array as an input to fully connected layer.  \n",
    "Theorically, convolution layers identify features.  \n",
    "Fully Connected Layer classify input using all identified features.  \n",
    "CNN is supervised learning, by giving target value, CNN will use back propagation to optimize parameters at convolution layers, fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Optimization\n",
    "The purpose of training is to optimize parameters of filter and parameters of fully connected layer(FC).  \n",
    "Initially we give random value for these parameters, but the CNN keep update parameters using backpropagation in order to have meaningful filters an FC.  \n",
    "Yes, initially the CNN even don't know which filter to have (head, tail, diagonal connector), the CNN will automatically know which filter to have during training.  \n",
    "By minimizing the difference between target and output, the CNN will eventually have meaningful filters (head, tail, diagonal connector, round, triangle) and also meaningful classifier with these feature maps.  \n",
    "Parameters (W in the below picture) can be optimized using gradience descent algorithm by minimizing difference between target and output (Loss in the below picture). The local minimum value of Loss is where derivative Loss with respect to W is 0. Therefore everytime we update W, we slightly update W with negative direction of derivative and find local minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/sgd.png\" width=\"300\" height=\"150\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/sgd.png\", width=300, height=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation\n",
    "Let's implement CNN using Tensorflow for identify handwritten number in MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/practice_cnn.png\" width=\"800\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/practice_cnn.png\", width=800, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data has **60000** samples  \n",
    "test data has **10000** samples   \n",
    "every data is **28 * 28** pixels  \n",
    "\n",
    "below image shows 28*28 pixel image sample for hand written number '0' from MNIST data.  \n",
    "MNIST is gray scale image [0 to 255] for hand written number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![0 from MNIST](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mnist_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train data into train and validation data\n",
    "Split train data into train data and validation data, in order to check validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val  = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val  = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 50000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data has 10000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_val.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 * 28 pixels has gray scale value from **0** to **255**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each train data has its label **0** to **9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data has **10000** samples  \n",
    "every test data is **28 * 28** image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 10000 samples\n",
      "every test data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"test data has \" + str(x_test.shape[0]) + \" samples\")\n",
    "print(\"every test data is \" + str(x_test.shape[1]) \n",
    "      + \" * \" + str(x_test.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "In order to fully connect all pixels to hidden layer,  \n",
    "we will reshape (28, 28) into (28x28,1) shape.  \n",
    "It means we flatten row x column shape to an array having 28x28 (756) items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_train = np.reshape(x_train, (50000,28,28,1))\n",
    "x_val = np.reshape(x_val, (10000,28,28,1))\n",
    "x_test = np.reshape(x_test, (10000,28,28,1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data\n",
    "normalization usually helps faster learning speed, better performance  \n",
    "by reducing variance and giving same range to all input features.  \n",
    "since MNIST data set all input has 0 to 255, normalization only helps reducing variances.  \n",
    "it turned out normalization is better than standardization for MNIST data with my MLP architeture,    \n",
    "I believe this is because relu handles 0 differently on both feed forward and back propagation.  \n",
    "handling 0 differently is important for MNIST, since 1-255 means there is some hand written,  \n",
    "while 0 means no hand written on that pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label to one hot encoding value\n",
    "In order to measure difference between softmax output and target,  \n",
    "target value need to be one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement CNN tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/practice_cnn.png\" width=\"800\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/practice_cnn.png\", width=800, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use image itself (28 x 28) as input of CNN.\n",
    "Target will be number between 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize parameters near 0.  \n",
    "The point for using **truncated normal** is to overcome saturation of sigmoid in softmax (where if the value is too big/small, the neuron stops learning).  \n",
    "tf.truncated_normal() selects random numbers from a normal distribution whose mean is close to 0 and values are close to 0 Ex. -0.1 to 0.1.  \n",
    "It's called truncated because your cutting off the tails from a normal distribution.  \n",
    "**tf.random_normal()** selects random numbers from a normal distribution whose mean is close to 0; however the values can be a bit further apart. Ex. -2 to 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Same padding** means the size of output feature-maps are the same as the input feature-maps.  \n",
    "For example, our MNIST has (28x28) shape, so the output will also (28,28) shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Convolutional Layer has 16 filters with size 5 by 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 16])\n",
    "b_conv1 = bias_variable([16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Relu activation funtion. Activation function brings non linearity in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Convolutional Layer, we apply Pooling layer to reduce activation map size.  \n",
    "Pooling layer will reduce parameters and control overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Max Pooling, now we have (14,14) input shape.  \n",
    "Here we have second Convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 16, 32])\n",
    "b_conv2 = bias_variable([32])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Max Pooling, now we have (7,7) input shape.\n",
    "#### FC (Fully Connected Layer)\n",
    "here is FC where we use activation maps from CONV as features for digit classification.  \n",
    "You can find, we flatten the activation map pixels to one array in order to input to FC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 32, 128])\n",
    "b_fc1 = bias_variable([128])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*32])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FC has two hidden layers. first hidden layer has 128 nodes, and second hidden layer has 10 nodes in order to match with our target range 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([128, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cross entropy as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Adam Optimizer as our parameters optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy will be from below code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test\n",
    "Here we go.  \n",
    "We will perform 3 epoches.  \n",
    "Using Mini Batch, we will optimize parameters everytime we pass 500 train data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: training accuracy: 0.086\n",
      "step 10: training accuracy: 0.61\n",
      "step 20: training accuracy: 0.754\n",
      "step 30: training accuracy: 0.854\n",
      "step 40: training accuracy: 0.874\n",
      "step 50: training accuracy: 0.918\n",
      "step 60: training accuracy: 0.916\n",
      "step 70: training accuracy: 0.946\n",
      "step 80: training accuracy: 0.944\n",
      "step 90: training accuracy: 0.95\n",
      "validation accuracy: 0.9508\n",
      "step 0: training accuracy: 0.952\n",
      "step 10: training accuracy: 0.952\n",
      "step 20: training accuracy: 0.94\n",
      "step 30: training accuracy: 0.944\n",
      "step 40: training accuracy: 0.952\n",
      "step 50: training accuracy: 0.97\n",
      "step 60: training accuracy: 0.968\n",
      "step 70: training accuracy: 0.972\n",
      "step 80: training accuracy: 0.978\n",
      "step 90: training accuracy: 0.966\n",
      "validation accuracy: 0.9703\n",
      "step 0: training accuracy: 0.968\n",
      "step 10: training accuracy: 0.974\n",
      "step 20: training accuracy: 0.96\n",
      "step 30: training accuracy: 0.962\n",
      "step 40: training accuracy: 0.968\n",
      "step 50: training accuracy: 0.978\n",
      "step 60: training accuracy: 0.978\n",
      "step 70: training accuracy: 0.982\n",
      "step 80: training accuracy: 0.984\n",
      "step 90: training accuracy: 0.972\n",
      "validation accuracy: 0.9765\n",
      "test accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 3\n",
    "batch_size = 500\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    tf.set_random_seed(777)\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0; end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            if i%10 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x:x_train[start: end], y_: y_train[start: end]})\n",
    "                print(\"step \"+ str(i) + \": training accuracy: \"+str(train_accuracy))\n",
    "            train_step.run(feed_dict={x:x_train[start: end], y_: y_train[start: end]})\n",
    "            start += batch_size; end += batch_size    \n",
    "        \n",
    "        # Validate model\n",
    "        val_accuracy = accuracy.eval(feed_dict={x:x_val, y_: y_val})\n",
    "        print(\"validation accuracy: \"+str(val_accuracy))\n",
    "        \n",
    "    test_accuracy = accuracy.eval(feed_dict={x:x_test, y_: y_test}) \n",
    "    print(\"test accuracy: \"+str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got test accuracy **97.7%** with even just **3** epoches.\n",
    "I wish you enjoyed learning CNN. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
